{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybRMS9D8sKxN",
        "outputId": "b6f221f8-9b87-4098-fe38-fdba1c2637d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYrEkniYsNpf",
        "outputId": "9685cc7e-6858-47ab-a1a7-35be8f02f48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-r-------- 1 root root 220400553 Sep  4 09:38 /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "###!@0 START INIT ENVIRONMENT\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls -al /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!tar xf /content/drive/Shareddrives/DA231-2023/assignments/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
        "###!@0 END INIT ENVIRONMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "FnwmTMqdsPra",
        "outputId": "438d3b1d-6f46-4369-cd8a-0e1fe6b2c19a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7914480e7130>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d00ca53f0243:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>StockPrediction</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#######################################\n",
        "###!@1 START OF PYSPARK INIT\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import SparkSession\n",
        "input_type = 'sample'\n",
        "spark = SparkSession.builder\\\n",
        "         .master(\"local\")\\\n",
        "         .appName(\"StockPrediction\")\\\n",
        "         .config('spark.ui.port', '4050')\\\n",
        "         .getOrCreate()\n",
        "spark\n",
        "# Spark is ready to go within Colab!\n",
        "###!@1 END OF PYSPARK INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1dEyxhTsTUN"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
        "from pyspark.sql import functions, Row\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import input_file_name, substring, regexp_extract, \\\n",
        "                                  col, regexp_replace, udf,date_format, \\\n",
        "                                  first, last, quarter, year, lag, sum\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLUIa3i9shiy"
      },
      "source": [
        "## Master instrument data file which is used to capture data from indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Wb5iI8vZct",
        "outputId": "2c117ba0-fa8b-4f06-b3e7-7ca165399807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-------------------+-------------------+-------------------+-------------+\n",
            "|                 _c0|2022-09-30 00:00:00|2022-12-31 00:00:00|2023-03-31 00:00:00|2023-06-30 00:00:00|tradingsymbol|\n",
            "+--------------------+-------------------+-------------------+-------------------+-------------------+-------------+\n",
            "|Tax Effect Of Unu...|                0.0|        -42760000.0|        -32000000.0|                0.0|        GEPIL|\n",
            "|  Tax Rate For Calcs|                0.0|                0.4|                0.4|                0.0|        GEPIL|\n",
            "|   Normalized EBITDA|       -856900000.0|           500000.0|       -940200000.0|      -1148500000.0|        GEPIL|\n",
            "| Total Unusual Items|                0.0|       -106900000.0|        -80000000.0|                0.0|        GEPIL|\n",
            "|Total Unusual Ite...|                0.0|       -106900000.0|        -80000000.0|                0.0|        GEPIL|\n",
            "|Net Income From C...|      -1125700000.0|      -1392900000.0|      -1296900000.0|      -1357900000.0|        GEPIL|\n",
            "|Reconciled Deprec...|        117800000.0|         50300000.0|         57200000.0|         49600000.0|        GEPIL|\n",
            "|Reconciled Cost O...|       3952600000.0|       3713900000.0|       2845100000.0|       3914600000.0|        GEPIL|\n",
            "|              EBITDA|       -856900000.0|       -106400000.0|      -1020200000.0|      -1148500000.0|        GEPIL|\n",
            "|                EBIT|       -974700000.0|       -156700000.0|      -1077400000.0|      -1198100000.0|        GEPIL|\n",
            "| Net Interest Income|       -151000000.0|       -138500000.0|       -292000000.0|       -159800000.0|        GEPIL|\n",
            "|    Interest Expense|        151000000.0|        138500000.0|        182000000.0|        159800000.0|        GEPIL|\n",
            "|   Normalized Income|      -1125700000.0|      -1328760000.0|      -1248900000.0|      -1357900000.0|        GEPIL|\n",
            "|Net Income From C...|      -1125700000.0|      -1392900000.0|      -1296900000.0|      -1357900000.0|        GEPIL|\n",
            "|      Total Expenses|       5576000000.0|       5501400000.0|       3891000000.0|       5606200000.0|        GEPIL|\n",
            "|Diluted Average S...|         67205970.0|         67224903.0|         67207894.0|         67222772.0|        GEPIL|\n",
            "|Basic Average Shares|         67205970.0|         67224903.0|         67207894.0|         67222772.0|        GEPIL|\n",
            "|         Diluted EPS|             -16.75|             -20.72|              -19.3|              -20.2|        GEPIL|\n",
            "|           Basic EPS|             -16.75|             -20.72|              -19.3|              -20.2|        GEPIL|\n",
            "|Diluted NI Availt...|      -1125700000.0|      -1392900000.0|      -1296900000.0|      -1357900000.0|        GEPIL|\n",
            "+--------------------+-------------------+-------------------+-------------------+-------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "{datetime.datetime(2023, 6, 30, 0, 0), datetime.datetime(2022, 11, 30, 0, 0), datetime.datetime(2023, 7, 31, 0, 0), datetime.datetime(2023, 8, 31, 0, 0), datetime.datetime(2023, 3, 31, 0, 0), datetime.datetime(2022, 8, 31, 0, 0), datetime.datetime(2023, 5, 31, 0, 0), datetime.datetime(2022, 10, 31, 0, 0), datetime.datetime(2023, 4, 30, 0, 0), datetime.datetime(2022, 9, 30, 0, 0), datetime.datetime(2023, 9, 30, 0, 0), datetime.datetime(2023, 2, 28, 0, 0), datetime.datetime(2023, 1, 31, 0, 0), datetime.datetime(2022, 12, 31, 0, 0)}\n"
          ]
        }
      ],
      "source": [
        "finance_data_path = \"/content/drive/MyDrive/DataEngineeringAtScale/finance-data-2/*.csv\"\n",
        "finance_data_file_list = glob(finance_data_path)\n",
        "\n",
        "quarters = set()\n",
        "\n",
        "for finance_data_file in finance_data_file_list:\n",
        "  #finance_data = spark.read.csv(finance_data_file_list[0], header = True)\n",
        "  finance_data = spark.read.option(\"delimiter\", \"\\t\") \\\n",
        "                  .option(\"header\", \"true\") \\\n",
        "                  .csv(finance_data_file)\n",
        "  if (not finance_data.columns):\n",
        "    print(\"name: \", finance_data_file)\n",
        "  else:\n",
        "    colnames = finance_data.columns\n",
        "    for colname in colnames[1:-1]:\n",
        "      quarters.add(datetime.strptime(colname, '%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "finance_data = spark.read.option(\"delimiter\", \"\\t\") \\\n",
        "                  .option(\"header\", \"true\") \\\n",
        "                  .csv(finance_data_file_list[0])\n",
        "finance_data.show()\n",
        "\n",
        "print(quarters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFdmlm3mshJW",
        "outputId": "c9bdff9b-33ee-4030-fdb4-758ab4919a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- macro: string (nullable = true)\n",
            " |-- sector: string (nullable = true)\n",
            " |-- industry: string (nullable = true)\n",
            " |-- basicIndustry: string (nullable = true)\n",
            " |-- delisted: string (nullable = true)\n",
            " |-- isETFSec: string (nullable = true)\n",
            " |-- tradingindex: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------------+--------------------+-------------+\n",
            "|exchange_token|instrument_token|                name|tradingsymbol|\n",
            "+--------------+----------------+--------------------+-------------+\n",
            "|        543151|       139046660|NIPPON INDIA MUTU...|        08ABB|\n",
            "|        543170|       139051524|NIPPON INDIA MUTU...|        08ADD|\n",
            "|        543145|       139045124|NIPPON INDIA MUTU...|        08ADR|\n",
            "|        543153|       139047172|NIPPON INDIA MUTU...|        08AGG|\n",
            "|        543147|       139045636|NIPPON INDIA MUTU...|        08AMD|\n",
            "|        543148|       139045892|NIPPON INDIA MUTU...|        08AMR|\n",
            "|        543155|       139047684|NIPPON INDIA MUTU...|        08AQD|\n",
            "|        543168|       139051012|NIPPON INDIA MUTU...|        08AQR|\n",
            "|        543149|       139046148|NIPPON INDIA MUTU...|        08BPB|\n",
            "|        543156|       139047940|NIPPON INDIA MUTU...|        08DPD|\n",
            "|        543169|       139051268|NIPPON INDIA MUTU...|        08DPR|\n",
            "|        543150|       139046404|NIPPON INDIA MUTU...|        08GPG|\n",
            "|        543144|       139044868|NIPPON INDIA MUTU...|        08MPD|\n",
            "|        543146|       139045380|NIPPON INDIA MUTU...|        08MPR|\n",
            "|        543152|       139046916|NIPPON INDIA MUTU...|        08QPD|\n",
            "|        543154|       139047428|NIPPON INDIA MUTU...|        08QPR|\n",
            "|        543179|       139053828|NIPPON INDIA MUTU...|        11ADD|\n",
            "|        543180|       139054084|NIPPON INDIA MUTU...|        11ADR|\n",
            "|        543181|       139054340|NIPPON INDIA MUTU...|        11AGG|\n",
            "|        543182|       139054596|NIPPON INDIA MUTU...|        11AMD|\n",
            "+--------------+----------------+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instrument_data_path = \"/content/drive/MyDrive/DataEngineeringAtScale/instruments-data.txt\"\n",
        "\n",
        "data = []\n",
        "eq_search_pattern = r\"'instrument_type': 'EQ'\"\n",
        "bse_search_pattern = r\"'exchange': 'BSE'\"\n",
        "name_search_pattern = r\"name':\\s*'([^']+)'\"\n",
        "\n",
        "import json\n",
        "\n",
        "instrument_data = pd.DataFrame(columns=[\"instrument_token\", \"exchange_token\",\n",
        "                                        \"tradingsymbol\"])\n",
        "eq_data = []\n",
        "\n",
        "with open(instrument_data_path, 'r') as instruments_data_f:\n",
        "    for data_str in instruments_data_f:\n",
        "      bse_match= re.search(bse_search_pattern, data_str)\n",
        "      if bse_match:\n",
        "          equity_match = re.search(eq_search_pattern, data_str)\n",
        "          if equity_match:\n",
        "              name_match = re.search(name_search_pattern, data_str)\n",
        "              if name_match:\n",
        "                  name = name_match.group(1)\n",
        "\n",
        "                  # Find the position of 'datetime' and the comma after it\n",
        "                  datetime_index = data_str.find(\"name\")\n",
        "                  comma_after_datetime_index = data_str.find(\"',\", datetime_index)\n",
        "                  data_str = data_str[:datetime_index-1] + data_str[comma_after_datetime_index+2:]\n",
        "\n",
        "                  # replace ' with \"\n",
        "                  data_str = data_str.replace(\"'\", \"\\\"\")\n",
        "\n",
        "                  # load the dictionary\n",
        "                  data = json.loads(data_str)\n",
        "\n",
        "                  eq_data.append({'name': name, \\\n",
        "                                   'instrument_token': data['instrument_token'], \\\n",
        "                                   'exchange_token': data['exchange_token'], \\\n",
        "                                   'tradingsymbol': data['tradingsymbol'], \\\n",
        "                                  })\n",
        "\n",
        "def get_finance_data_file_name(symbolname) -> str:\n",
        "    filename = symbolname + \"_finance.csv\"\n",
        "\n",
        "    #check for filenane\n",
        "    for file_n in finance_data_file_list:\n",
        "      if file_n.find(filename) != -1:\n",
        "        return file_n\n",
        "\n",
        "    return \"NA\"\n",
        "\n",
        "# Define a function to process each row and add a new column\n",
        "def process_row(a2):\n",
        "    path = get_finance_data_file_name(a2[\"name\"])\n",
        "    return a2 + (path, )\n",
        "\n",
        "process_row_udf = udf(process_row, StringType())\n",
        "\n",
        "# company data\n",
        "company_df = spark.read.csv(\"/content/drive/MyDrive/DataEngineeringAtScale/trading-industry-2.csv\", header=True, inferSchema=True)\n",
        "company_df = company_df.filter((col(\"delisted\") == False) & (col(\"delisted\").isNotNull()))\n",
        "company_df = company_df.drop(\"name\")\n",
        "company_df.printSchema()\n",
        "#company_df.show()\n",
        "\n",
        "#instrument data\n",
        "instrument_data_df = spark.createDataFrame(eq_data)\n",
        "instrument_data_df.show()\n",
        "\n",
        "industry_data_df = instrument_data_df.join(company_df,\n",
        "                        company_df.tradingindex == instrument_data_df.tradingsymbol,\n",
        "                        how='inner')\n",
        "industry_data_df = industry_data_df.drop(\"tradingsymbol\", \"exchange_token\", \"instrument_token\")\n",
        "industry_data_df = industry_data_df.rdd.map(process_row).toDF(industry_data_df.columns + [\"finance-data-path\"])\n",
        "#industry_data_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbaa-5Jwtm6"
      },
      "source": [
        "## Stock price data (Hourly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6keSZHuvwmVK"
      },
      "outputs": [],
      "source": [
        "Stock_DataPath = \"/content/drive/MyDrive/DataEngineeringAtScale/hourly-data/*.txt\"\n",
        "file_list = glob(Stock_DataPath)\n",
        "\n",
        "schema = StructType([StructField(\"date\",TimestampType(),True),\n",
        "                     StructField(\"open\",DoubleType(),True),\n",
        "                     StructField(\"high\",DoubleType(),True),\n",
        "                     StructField(\"low\",DoubleType(),True),\n",
        "                     StructField(\"close\",DoubleType(),True),\n",
        "                     StructField(\"volume\",DoubleType(),True)])\n",
        "\n",
        "hourly_data_df = spark.read \\\n",
        "    .option(\"header\", \"false\") \\\n",
        "    .option(\"inferSchema\", \"false\") \\\n",
        "    .schema(schema) \\\n",
        "    .json(Stock_DataPath) \\\n",
        "    .withColumn(\"symbol\", regexp_extract(input_file_name(), \"/([^/]+)$\", 1))\n",
        "\n",
        "hourly_data_df = hourly_data_df.withColumnRenamed(\"symbol\", \"name\")\n",
        "hourly_data_df = hourly_data_df.withColumn(\"name\", regexp_replace(col(\"name\"), r\"_hourly_data_bse\\.txt\", \"\"))\n",
        "hourly_data_df = hourly_data_df.withColumn(\"name\", regexp_replace(col(\"name\"), \"%20\", \" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1P9_ND2w-eR",
        "outputId": "84cb374e-62dc-40b3-8f62-9f93c3ed84e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------+------+------+------+-------+--------------------+\n",
            "|               date|  open|  high|   low| close| volume|                name|\n",
            "+-------------------+------+------+------+------+-------+--------------------+\n",
            "|2022-10-06 09:15:00| 323.3| 326.6| 321.8|323.05|89026.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 10:15:00|323.05| 324.8|322.85| 324.8| 6803.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 11:15:00|324.45| 325.0| 323.5| 323.7| 4044.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 12:15:00| 323.7|324.55|323.25| 323.6| 3927.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 13:15:00| 323.6|323.75| 320.6|320.95| 8709.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 14:15:00|320.95|323.05| 320.1|321.65| 9556.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-06 15:15:00|321.65|322.55| 320.9|322.55| 5936.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 09:15:00|324.15|324.15| 316.4|320.05|21319.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 10:15:00|320.05|320.05| 318.1| 318.5| 4467.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 11:15:00| 318.5| 319.2|317.75| 318.6| 8707.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 12:15:00| 318.6|319.05| 318.0| 318.1| 3705.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 13:15:00| 318.1| 321.7| 317.5| 321.2| 4366.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 14:15:00| 321.2|321.25|319.65| 320.3| 7629.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-07 15:15:00| 320.3| 321.0| 320.0| 320.0| 4828.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 09:15:00| 315.1|317.65|313.25|314.75| 8231.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 10:15:00|314.75|316.35| 314.5| 315.9| 3549.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 11:15:00| 315.9| 315.9| 314.1| 315.2| 2269.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 12:15:00| 315.2|316.75| 315.2|316.45| 1012.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 13:15:00|316.45|318.25| 315.8| 318.0| 1991.0|CHAMBAL FERTILISE...|\n",
            "|2022-10-10 14:15:00| 318.0| 319.0| 317.8| 319.0| 3443.0|CHAMBAL FERTILISE...|\n",
            "+-------------------+------+------+------+------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hourly_data_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11_Dzt-3xhuQ"
      },
      "source": [
        "## Identify the quarterly gain per stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKciqNAjxfWt"
      },
      "outputs": [],
      "source": [
        "# Convert 'Date' column to a date format\n",
        "#hourly_data_df = hourly_data_df.withColumn(\"date\", date_format(\"date\", \"yyyy-MM-dd HH::MM::SS\"))\n",
        "\n",
        "# Extract date to group data by date\n",
        "hourly_data_day_df = hourly_data_df.withColumn(\"day\", substring(\"date\", 1, 10))\n",
        "\n",
        "# Calculate the daily stock price change for each stock\n",
        "daily_data_aggregated_vol_df = hourly_data_day_df.groupBy(\"day\", \"name\") \\\n",
        "    .agg({\"open\": \"first\", \"close\": \"last\", \"volume\": \"sum\"}) \\\n",
        "    .withColumnRenamed(\"first(open)\", \"open\") \\\n",
        "    .withColumnRenamed(\"last(close)\", \"close\") \\\n",
        "    .withColumnRenamed(\"sum(volume)\", \"total_volume\")\n",
        "\n",
        "daily_data_aggregated_vol_df.show(10)\n",
        "\n",
        "# Extract year and quarter from the 'day' column\n",
        "quarterly_data = daily_data_aggregated_vol_df.withColumn(\"year\", year(\"day\")).withColumn(\"quarter\", quarter(\"day\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZBJKU3qqz0E"
      },
      "outputs": [],
      "source": [
        "# Group by 'name', 'Year', and 'Quarter', aggregating open price, close price, and total volume\n",
        "quarterly_aggregated_df = quarterly_data.groupBy(\"name\", \"year\", \"quarter\") \\\n",
        "    .agg(first(\"open\").alias(\"quarterly_open\"), \\\n",
        "         last(\"close\").alias(\"quarterly_close\"), \\\n",
        "         sum(\"total_volume\").alias(\"total_volume\"))\n",
        "\n",
        "# Show the DataFrame with the calculated price change for each year and each quarter\n",
        "quarterly_aggregated_df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36UQdDmQFY2O"
      },
      "source": [
        "Find the hour of the day with maximum gain and maximum volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KA0KJ4BFTp5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "def get_hour_day_from_df(hourly_df):\n",
        "  # Extract date and hour from the 'date' column\n",
        "  hourly_date_timestamp_df = hourly_df.withColumn(\"date\", F.to_timestamp(\"date\"))  # Ensure 'date' is in timestamp format\n",
        "  hourly_date_day_df = hourly_date_timestamp_df.withColumn(\"day\", F.to_date(\"date\"))\n",
        "  hourly_day_hour_df = hourly_date_day_df.withColumn(\"hour\", F.hour(\"date\"))\n",
        "  return hourly_day_hour_df\n",
        "\n",
        "def get_hour_max_volume():\n",
        "  hourly_day_hour_df = get_hour_day_from_df(hourly_data_df)\n",
        "\n",
        "  # Window specification to find max volume hour for each day and stock\n",
        "  window_max_volume = Window.partitionBy(\"day\", \"name\").orderBy(F.desc(\"volume\"))\n",
        "\n",
        "  # Find the hour with the maximum volume for each day and each stock\n",
        "  max_volume_df = hourly_day_hour_df.withColumn(\"max_volume_rank\", F.rank().over(window_max_volume)) \\\n",
        "      .filter(F.col(\"max_volume_rank\") == 1) \\\n",
        "      .drop(\"max_volume_rank\")\n",
        "\n",
        "  return max_volume_df\n",
        "\n",
        "def get_hour_max_price_gain(hourly_data_df):\n",
        "  hourly_day_hour_df = get_hour_day_from_df(hourly_data_df)\n",
        "\n",
        "  # Window specification to find max gain hour within each day and stock's max volume hour\n",
        "  window_max_gain = Window.partitionBy(\"day\", \"name\", \"hour\")\n",
        "\n",
        "  # Calculate max gain for each 'day', 'name', and 'hour'\n",
        "  max_gain_price_df = hourly_day_hour_df.withColumn(\"max_gain\", \\\n",
        "                                                    F.max(\"high\").over(window_max_gain) - F.min(\"low\").over(window_max_gain))\n",
        "\n",
        "  # Calculate maximum gain percent\n",
        "  max_gain_percent_df = max_gain_price_df.withColumn(\"max_gain_percent\", (F.col(\"max_gain\") / F.col(\"low\") * 100)).drop(\"max_gain\")\n",
        "\n",
        "\n",
        "  # Select the rows with the maximum gain for each 'day' and 'name'\n",
        "  max_gain_hour_df = max_gain_price_df.withColumn(\"rn\", \\\n",
        "                                                  F.row_number().over(Window.partitionBy(\"day\", \"name\").orderBy(F.desc(\"max_gain\")))) \\\n",
        "                                    .filter(F.col(\"rn\") == 1) \\\n",
        "                                    .drop(\"rn\")\n",
        "\n",
        "  return max_gain_hour_df\n",
        "\n",
        "max_gain_price_df = get_hour_max_price_gain(hourly_data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJfAaxVMHw8I"
      },
      "outputs": [],
      "source": [
        "max_gain_price_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "832zp7rjvBIZ"
      },
      "outputs": [],
      "source": [
        "# Group by hour and count the number of stocks with +ve / -ve gain for each hour\n",
        "positive_gains_by_hour = max_gain_price_df.groupBy(\"hour\").agg(F.countDistinct(\"name\").alias(\"count_by_hour\"))\n",
        "\n",
        "# Identify the hour with the most positive gains\n",
        "hour_with_most_positive_gains = positive_gains_by_hour.orderBy(F.desc(\"count_by_hour\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hour_with_most_positive_gains.show()"
      ],
      "metadata": {
        "id": "lGuzaQ1ETP3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHD_OPs-w0t_"
      },
      "outputs": [],
      "source": [
        "def calculate_moving_avg(df, n_days, column, tgt_column):\n",
        "  # Create a window specification\n",
        "  days = lambda i: i * 86400\n",
        "  window_spec = Window.partitionBy(\"name\").orderBy(F.col(\"day\").cast(\"timestamp\").cast(\"long\")).rowsBetween(-n_days, 0)\n",
        "\n",
        "  # Calculate the moving average of closing price over 'n' days\n",
        "  df_with_ma = df.withColumn(tgt_column, F.avg(column).over(window_spec))\n",
        "\n",
        "  # Replace null values with the 'close' price itself per name\n",
        "  df_with_ma = df_with_ma.withColumn(tgt_column, F.coalesce(col(tgt_column), col(\"close\")))\n",
        "\n",
        "\n",
        "  return df_with_ma\n",
        "\n",
        "#daily_data_aggregated_vol_df = calculate_moving_avg(daily_data_aggregated_vol_df, 20, \"close\", \"moving_avg_closing_price_20\")\n",
        "#daily_data_aggregated_vol_df = calculate_moving_avg(daily_data_aggregated_vol_df, 50, \"close\", \"moving_avg_closing_price_50\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "daily_data_aggregated_vol_df.show(50)"
      ],
      "metadata": {
        "id": "Obp6YdkJnjSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it_sw_companies_df = industry_data_df[industry_data_df[\"industry\"] == \"IT - Software\"]"
      ],
      "metadata": {
        "id": "weU9QbjJiOXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select distinct 'name' from df\n",
        "it_distinct_names = it_sw_companies_df.select('name').distinct()\n",
        "it_sw_daily_data_df = daily_data_aggregated_vol_df.join(it_distinct_names, 'name', 'inner')\n",
        "it_sw_daily_data_df.show()"
      ],
      "metadata": {
        "id": "F6eyq1PajKDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it_sw_daily_data_df = it_sw_daily_data_df.drop(\"moving_avg_closing_price_20\")\n",
        "it_sw_ma_df = calculate_moving_avg(it_sw_daily_data_df, 10, \"close\", \"moving_average_10_days_new\")\n",
        "it_sw_ma_df.show(20)"
      ],
      "metadata": {
        "id": "tXxJ28Ic5KJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_days_spent_above_below_ma(df, ma_column):\n",
        "  # Calculate the difference between close and the moving average\n",
        "  price_diff_df = df.withColumn('price_diff_ma', F.col('close') - F.col(ma_column))\n",
        "\n",
        "  # Determine if the stock is above or below the moving average\n",
        "  price_diff_df = price_diff_df.withColumn('above_ma', F.when(F.col('price_diff_ma') > 0, 1).otherwise(0))\n",
        "  price_diff_df = price_diff_df.withColumn('below_ma', F.when(F.col('price_diff_ma') < 0, 1).otherwise(0))\n",
        "\n",
        "  # Define a window specification based on consecutive days\n",
        "  window_spec_above = Window.partitionBy('name').orderBy('day').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "  window_spec_below = Window.partitionBy('name', 'below_ma').orderBy('day').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "  # Use window functions to count the number of days spent above and below the moving average\n",
        "  result_df = price_diff_df.withColumn('days_above_ma', F.sum('above_ma').over(window_spec_above))\n",
        "  result_df = result_df.withColumn('days_below_ma', F.sum('below_ma').over(window_spec_above))\n",
        "\n",
        "  # Aggregate to get the total number of days spent above and below the moving average\n",
        "  agg_result = result_df.groupBy('name').agg(F.max('days_above_ma').alias('total_days_above_ma'), F.max('days_below_ma').alias('total_days_below_ma'))\n",
        "  return agg_result\n"
      ],
      "metadata": {
        "id": "THg3z_-BtBFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2k-X13sMYrX"
      },
      "outputs": [],
      "source": [
        "ma_10days = calculate_days_spent_above_below_ma(it_sw_ma_df, \"moving_average_10_days\")\n",
        "#ma_50days = calculate_days_spent_above_below_ma(daily_data_aggregated_vol_df, \"moving_avg_closing_price_20\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ma_10days = ma_10days.withColumn(\"ratio_above_below_ma\",\n",
        "                             col(\"total_days_above_ma\") / col(\"total_days_below_ma\"))\n",
        "companies_trading_above_ma = ma_10days.filter(col(\"ratio_above_below_ma\") > 1).select(\"name\").distinct().count()\n",
        "print(\"Out of {0} companies {1} are trading above ma for longer time\".format(ma_10days.count(), companies_trading_above_ma))"
      ],
      "metadata": {
        "id": "IUi6JYpUEe2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the stocks which have fundamental data captured\n",
        "industry_data_distinct_names = industry_data_df.select('name').distinct()\n",
        "filtered_data_df = daily_data_aggregated_vol_df.join(industry_data_distinct_names, 'name', 'inner')\n",
        "\n",
        "#Calculate moving averages\n",
        "filtered_data_ma_20_days_df = calculate_moving_avg(filtered_data_df, 20, \"close\", \"moving_average_20_days\")\n",
        "filtered_data_ma_20_days_analysis_df = calculate_days_spent_above_below_ma(filtered_data_ma_20_days_df, \"moving_average_20_days\")\n",
        "\n",
        "# Get the ratio on days spent above / below moving average\n",
        "filtered_data_ma_20_days_analysis = filtered_data_ma_20_days_analysis_df.withColumn(\"ratio_above_below_ma\",\n",
        "                             col(\"total_days_above_ma\") / col(\"total_days_below_ma\"))\n",
        "companies_trading_above_ma = filtered_data_ma_20_days_analysis.filter(col(\"ratio_above_below_ma\") > 1).select(\"name\").distinct().count()\n",
        "\n",
        "\n",
        "print(\"Out of {0} companies {1} are trading above ma for longer time\".format(filtered_data_ma_20_days_analysis.count(), companies_trading_above_ma))"
      ],
      "metadata": {
        "id": "TpU6zPhFKxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ma_20_analysis_df = industry_data_df.join(filtered_data_ma_20_days_analysis, filtered_data_ma_20_days_analysis[\"name\"] == industry_data_df[\"name\"])\n",
        "ma_20_analysis_df.show(10)\n"
      ],
      "metadata": {
        "id": "vdWlCIbIbdzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Aggregate counts for each industry status\n",
        "industry_counts = ma_20_analysis_df.groupBy('ratio_above_below_ma').count().collect()\n",
        "\n",
        "# Extracting counts for above and below MA\n",
        "above_ma_count = next(item['count'] for item in industry_counts if item['above_below_ma'] == 'Above MA')\n",
        "below_ma_count = next(item['count'] for item in industry_counts if item['above_below_ma'] == 'Below MA')\n",
        "\n",
        "# Creating a bar chart\n",
        "status = ['Above MA', 'Below MA']\n",
        "counts = [above_ma_count, below_ma_count]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(status, counts, color=['green', 'red'])\n",
        "plt.xlabel('Industry Status')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Industries Above and Below Moving Average')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FpzTwjUVeClC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RzTRkInZEeBg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}